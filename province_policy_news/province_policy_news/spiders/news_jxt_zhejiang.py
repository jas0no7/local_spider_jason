import copy
import re
from hashlib import md5
from lxml import html

import scrapy
from loguru import logger
from scrapy.utils.project import get_project_settings

from ..items import DataItem
from ..mydefine import get_attachment, get_now_date

settings = get_project_settings()


class EitdznewsSpider(scrapy.Spider):
    name = "news_jxt_zhejiang"
    allowed_domains = ["jxt.zj.gov.cn"]

    _from = "æµ™æ±Ÿçœç»æµå’Œä¿¡æ¯åŒ–å…"
    category = "æ”¿åºœç½‘ç«™"
    dupefilter_field = {"batch": "20240322"}

    custom_settings = {
        "DUPEFILTER_CLASS": "scrapy.dupefilters.RFPDupeFilter",
    }

    infoes = [
        {
            "url": "https://jxt.zj.gov.cn/col/col1659217/index.html?uid=5031616&pageNum=1",
            "label": "æ”¿ç­–æ³•è§„",

            # æ³¨æ„ï¼šåˆ—è¡¨å¹¶ä¸åœ¨ HTMLï¼Œè€Œæ˜¯åœ¨ script çš„ CDATA ä¸­
            "body_xpath": (
                '//div[@class="bt-box-1170 c1"] | '
                '//div[@class="wrapper_detail_text"] | '
                '//div[@class="article-content"]'
            ),

            "total": 6,
            "page": 1,
            "base_url": "https://jxt.zj.gov.cn/col/col1659217/index.html?uid=5031616&pageNum={}"
        },
    ]

    def start_requests(self):
        for info in self.infoes:
            yield scrapy.Request(
                url=info["url"],
                callback=self.parse_item,
                meta=copy.deepcopy(info),
                dont_filter=True
            )

    # --------------------------------------------------------
    # â­ å…³é”®ï¼šè§£æ <script type="text/xml"> é‡Œçš„ CDATA åˆ—è¡¨
    # --------------------------------------------------------
    def parse_item(self, response):
        logger.info("è¿›å…¥ parse_item")

        meta = response.meta
        page = meta["page"]
        total = meta["total"]

        # 1) æ‹¿åˆ° script é‡Œçš„ XML
        xml_text = response.xpath('//div[@id="5031616"]/script/text()').get()
        if not xml_text:
            logger.error("âŒ æœªæ‰¾åˆ° script XML å†…å®¹")
            return

        # 2) æŠ½å– CDATA å†…çš„ HTML æ®µè½
        records = re.findall(r'<!\[CDATA\[(.*?)\]\]>', xml_text, re.S)
        if not records:
            logger.error("âŒ XML ä¸­æœªæ‰¾åˆ° record CDATA å†…å®¹")
            return

        records_html = "".join(records)

        # 3) è½¬æˆ DOM æ ‘
        doc = html.fromstring(records_html)

        # 4) æ¯æ®µå®é™…ç»“æ„ï¼š<p class="lb-list"><a ...>æ ‡é¢˜</a><span>æ—¥æœŸ</span></p>
        for p in doc.xpath('//p[@class="lb-list"]'):
            relative_url = p.xpath('./a/@href')
            if not relative_url:
                continue

            detail_url = response.urljoin(relative_url[0])
            title = "".join(p.xpath('./a/text()')).strip()
            publish_time = "".join(p.xpath('./span/text()')).strip()

            logger.info(f"å‘ç°æ–‡ç« ï¼š{title} | {detail_url}")

            detail_meta = {
                "label": meta["label"],
                "title": title,
                "publish_time": publish_time,
                "body_xpath": meta["body_xpath"]
            }

            yield scrapy.Request(
                url=detail_url,
                callback=self.parse_detail,
                meta=copy.deepcopy(detail_meta),
                dont_filter=True
            )

        # --------------------------------------------------------
        # ğŸ”„ ç¿»é¡µé€»è¾‘
        # --------------------------------------------------------
        if False and page < total:
            next_page = page + 1
            next_url = meta["base_url"].format(next_page)

            logger.info(f"æŠ“å–ç¬¬ {next_page} é¡µï¼š{next_url}")

            next_meta = copy.deepcopy(meta)
            next_meta["page"] = next_page

            yield scrapy.Request(
                url=next_url,
                callback=self.parse_item,
                meta=next_meta,
                dont_filter=True
            )

    # --------------------------------------------------------
    # â­ è¯¦æƒ…é¡µè§£æ
    # --------------------------------------------------------
    def parse_detail(self, response):
        meta = response.meta

        method = response.request.method
        body = response.request.body.decode("utf-8") if response.request.body else ""
        url = response.url

        title = meta["title"] or response.xpath(
            '//meta[@name="ArticleTitle"]/@content'
        ).get()

        publish_time = meta["publish_time"]
        body_xpath = meta["body_xpath"]

        # æå–ä½œè€…
        author = (
            "".join(re.findall(r"æ¥æº[:ï¼š]\s*(.*?)<", response.text))
            or "".join(response.xpath('//meta[@name="Author"]/@content').getall())
        ).strip()

        # é™„ä»¶
        attachment_nodes = response.xpath(
            f'{body_xpath}//a[contains(@href, ".pdf") or contains(@href, ".doc") '
            'or contains(@href, ".docx") or contains(@href, ".wps")]'
        )
        attachments = get_attachment(attachment_nodes, url, self._from)

        # å†…å®¹è§£æ
        body_html = " ".join(response.xpath(body_xpath).extract())
        content = " ".join(response.xpath(f"{body_xpath}//text()").extract()).strip()
        images = [response.urljoin(i) for i in response.xpath(f"{body_xpath}//img/@src").extract()]

        yield DataItem({
            "_id": md5(f"{method}{url}{body}".encode("utf-8")).hexdigest(),
            "url": url,
            "spider_topic": settings.get("KAFKA_TOPIC", {}).get(self.name),
            "spider_from": self._from,
            "label": meta["label"],
            "category": self.category,

            "title": title,
            "author": author,
            "publish_time": publish_time,

            "body_html": body_html,
            "content": content,
            "images": images,
            "attachment": attachments,
            "spider_date": get_now_date(),
        })
